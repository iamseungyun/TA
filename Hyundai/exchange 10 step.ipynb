{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8851cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import pmdarima as pm\n",
    "import itertools\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, acf\n",
    "from statsmodels.graphics.tsaplots  import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.api import Holt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from arch import arch_model\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2e6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange = pd.read_csv('./data/exchange.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c7f77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange['date'] = pd.to_datetime(exchange['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0951ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasting_horizon = int(exchange.shape[0]*0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e737f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['0', '1', '2', '3', '4', '5', '6', 'OT']\n",
    "columns = ['Actual', 'ARIMA', 'ARIMA 2',\\\n",
    "           'Simple Exponential Smoothing','Holt\\'s Exponential Smoothing', \\\n",
    "          'Prophet', 'Prophet+ARIMA']\n",
    "\n",
    "\n",
    "results = {}\n",
    "results = pd.DataFrame(results, index=index, columns =columns)\n",
    "results = results.drop(['Actual'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97bd73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange0 = exchange[['date','0']]\n",
    "exchange1 = exchange[['date','1']]\n",
    "exchange2 = exchange[['date','2']]\n",
    "exchange3 = exchange[['date','3']]\n",
    "exchange4 = exchange[['date','4']]\n",
    "exchange5 = exchange[['date','5']]\n",
    "exchange6 = exchange[['date','6']]\n",
    "exchangeOT = exchange[['date','OT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "631ba512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_20708\\1671268679.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange0.rename(columns={'date':'ds', '0':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_20708\\1671268679.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange1.rename(columns={'date':'ds', '1':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_20708\\1671268679.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange2.rename(columns={'date':'ds', '2':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_20708\\1671268679.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange3.rename(columns={'date':'ds', '3':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_20708\\1671268679.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange4.rename(columns={'date':'ds', '4':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_20708\\1671268679.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange5.rename(columns={'date':'ds', '5':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_20708\\1671268679.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange6.rename(columns={'date':'ds', '6':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_20708\\1671268679.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchangeOT.rename(columns={'date':'ds', 'OT':'y'}, inplace =True)\n"
     ]
    }
   ],
   "source": [
    "exchange0.rename(columns={'date':'ds', '0':'y'}, inplace =True)\n",
    "exchange1.rename(columns={'date':'ds', '1':'y'}, inplace =True)\n",
    "exchange2.rename(columns={'date':'ds', '2':'y'}, inplace =True)\n",
    "exchange3.rename(columns={'date':'ds', '3':'y'}, inplace =True)\n",
    "exchange4.rename(columns={'date':'ds', '4':'y'}, inplace =True)\n",
    "exchange5.rename(columns={'date':'ds', '5':'y'}, inplace =True)\n",
    "exchange6.rename(columns={'date':'ds', '6':'y'}, inplace =True)\n",
    "exchangeOT.rename(columns={'date':'ds', 'OT':'y'}, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7f730",
   "metadata": {},
   "source": [
    "# 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f3f021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6364482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[0] = arima\n",
    "#results['ARIMA-GARCH'].iloc[0] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d5e9d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "#ARIMA(1,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[0] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[0] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a59ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-9])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(10).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-9])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(10).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-9], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(10).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[0] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[0] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[0] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3eb870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:43:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:43:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:43:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:43:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:43:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:43:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:44:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:44:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:44:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:44:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:44:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:44:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:45:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:45:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:45:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:45:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:45:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:45:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:46:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:46:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:46:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:46:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:46:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:46:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:47:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:47:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:47:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:47:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:47:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:47:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:48:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:48:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:48:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:49:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:49:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:49:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:50:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:50:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:50:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:50:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:50:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:50:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:51:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:51:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:51:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:51:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:52:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:52:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:52:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:53:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:53:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:53:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:53:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:53:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:53:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:53:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:54:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:54:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:54:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:54:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:54:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:54:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:54:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:55:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:55:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:55:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:55:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:55:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:55:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:56:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:56:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:56:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:56:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:56:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:56:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:57:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:57:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:57:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:57:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:57:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:57:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:57:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:58:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:58:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:58:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:58:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:58:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:58:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:58:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:59:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:59:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:59:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:59:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:59:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:59:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:00:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:00:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:00:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:00:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:00:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:00:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:00:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:01:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:01:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:01:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:02:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:02:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:02:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:02:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:02:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:02:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:02:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:03:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:03:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:03:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:04:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:04:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:04:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:04:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:04:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:04:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:03 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:05:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:06:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:06:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:06:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:06:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:06:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:06:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:07:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:07:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:07:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:07:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:07:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:07:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:08:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:08:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:08:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:08:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:08:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:08:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:09:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:09:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:09:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:09:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:09:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:09:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:10:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:10:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:10:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:10:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:10:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:10:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:11:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:11:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:11:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:11:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:11:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:11:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:12:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:12:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:12:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:12:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:12:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:12:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:13:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:13:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:13:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:13:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:13:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:13:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:14:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:14:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:14:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:14:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:14:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:14:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:16:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:16:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:16:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:16:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:16:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:16:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:17:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:17:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:17:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:17:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:17:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:17:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:18:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:18:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:18:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:18:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:18:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:18:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:19:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:19:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:19:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:19:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:19:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:19:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:19:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:19:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:20:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:20:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:20:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:20:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:20:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:20:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:21:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:21:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:21:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:21:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:21:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:21:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:22:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:22:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:22:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:22:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:22:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:22:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:23:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:23:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:23:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:23:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:23:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:23:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:24:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:24:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:24:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:24:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:24:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:24:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:24:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:25:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:25:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:25:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:25:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:25:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:25:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:26:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:26:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:26:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:26:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:26:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:26:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:27:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:27:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:27:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:27:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:27:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:27:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:27:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:28:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:28:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:28:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:28:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:28:56 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:28:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:29:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:29:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:30:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:30:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:31:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:31:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:31:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:35:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:35:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:35:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:37:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:37:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:37:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:37:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:37:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:37:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:58 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:52:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:53:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:53:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:53:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:57:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:57:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:57:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:04:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:04:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:04:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:08:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:08:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:08:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:08:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:08:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:08:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:09:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:09:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:09:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:09:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:09:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:09:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:10:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:10:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:10:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:11:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:11:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:11:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:11:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:11:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:11:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:11:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:11:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:12:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:12:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:12:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:12:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:12:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:12:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:13:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:13:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:13:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:13:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:13:44 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:13:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:14:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:14:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:14:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:15:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:15:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:15:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:15:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:15:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:16:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:16:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:16:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:16:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:16:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:16:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:16:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:17:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:17:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:17:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:17:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:17:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:17:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:18:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:18:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:18:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:18:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:18:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:18:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:18:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:19:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:19:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:19:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:19:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:19:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:20:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:20:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:20:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:20:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:20:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:20:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:21:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:21:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:21:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:21:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:21:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:21:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:21:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:22:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:22:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:22:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:22:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:22:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:22:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:23:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:23:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:23:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:23:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:23:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:23:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:24:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:24:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:24:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:24:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:24:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:24:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:25:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:25:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:25:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:25:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:25:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:25:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:26:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:26:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:26:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:26:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:26:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:27:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:27:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:27:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:27:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:27:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:27:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:27:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:28:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:28:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:29:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:29:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:29:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:29:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:29:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:29:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:30:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:30:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:30:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:30:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:30:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:30:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:31:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:31:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:31:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:31:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:31:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:31:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:32:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:32:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:32:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:32:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:32:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:32:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:33:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:33:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:33:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:33:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:33:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:33:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:34:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:34:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:34:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:34:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:34:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:34:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:37:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:37:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:37:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:37:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:37:51 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:37:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:38:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:38:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:38:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:38:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:38:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:38:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:39:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:39:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:39:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:39:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:39:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:39:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:40:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:40:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:40:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:40:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:40:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:40:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:44:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:44:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:44:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:45:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:45:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:45:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:45:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:45:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:45:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:46:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:46:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:46:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:47:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:47:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:47:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:47:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:47:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:47:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:48:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:48:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:48:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:49:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:49:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:49:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:49:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:49:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:49:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:50:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:50:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:50:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:50:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:50:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:50:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:52:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:52:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:52:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:52:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:52:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:52:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:52:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:52:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:54:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:54:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:54:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:54:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:55:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:55:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:55:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:55:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:55:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:55:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:56:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:56:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:56:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:56:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:56:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:56:53 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-10]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-9])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-9] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=10).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[0] = prophet\n",
    "results['Prophet+ARIMA'].iloc[0] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac004cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange0_10step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_10step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65620694",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b3ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[1] = arima\n",
    "#results['ARIMA-GARCH'].iloc[1] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f57d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[1] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[1] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4777950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-9])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(10).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-9])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(10).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-9], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(10).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[1] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[1] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[1] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13670b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-10]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-9])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-9] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=10).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[1] = prophet\n",
    "results['Prophet+ARIMA'].iloc[1] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f211a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange1_10step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb162da",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_10step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3b074",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[2] = arima\n",
    "#results['ARIMA-GARCH'].iloc[2] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(2,1,2)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(2,1,2)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (2,1,2)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[2] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[2] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e673ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-9])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(10).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-9])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(10).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-9], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(10).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[2] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[2] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[2] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-10]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-9])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-9] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=10).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[2] = prophet\n",
    "results['Prophet+ARIMA'].iloc[2] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange2_10step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_10step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e8fa6",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4475c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(3,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(3,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (3,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[3] = arima\n",
    "#results['ARIMA-GARCH'].iloc[3] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,3)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,3)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,3)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[3] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[3] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653515b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-9])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(10).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-9])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(10).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-9], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(10).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[3] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[3] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[3] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-10]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-9])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-9] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=10).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[3] = prophet\n",
    "results['Prophet+ARIMA'].iloc[3] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfad429",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange3_10step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ac366",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_10step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce946a",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6545129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[4] = arima\n",
    "#results['ARIMA-GARCH'].iloc[4] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86136ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,2,5)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,2,5)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,2,5)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[4] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[4] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-9])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(10).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-9])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(10).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-9], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(10).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[4] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[4] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[4] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-10]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-9])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-9] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=10).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[4] = prophet\n",
    "results['Prophet+ARIMA'].iloc[4] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange4_10step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_10step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45164944",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[5] = arima\n",
    "#results['ARIMA-GARCH'].iloc[5] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,3)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,3)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,3)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[5] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[5] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3cadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-9])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(10).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-9])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(10).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-9], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(10).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[5] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[5] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[5] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-10]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-9])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-9] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=10).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[5] = prophet\n",
    "results['Prophet+ARIMA'].iloc[5] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange5_10step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb957a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_10step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eeb0fa",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e6aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[6] = arima\n",
    "#results['ARIMA-GARCH'].iloc[6] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[6] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[6] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20174eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-9])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(10).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-9])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(10).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-9], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(10).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[6] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[6] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[6] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aae3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-10]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-9])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-9] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=10).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[6] = prophet\n",
    "results['Prophet+ARIMA'].iloc[6] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5590563",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange6_10step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_10step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f1426",
   "metadata": {},
   "source": [
    "# OT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchangeOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[7] = arima\n",
    "#results['ARIMA-GARCH'].iloc[7] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-9], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=10).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=10).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=10)\n",
    "#     predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[7] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[7] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-9])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(10).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-9])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(10).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-9], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(10).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[7] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[7] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[7] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-10]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-9])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-9] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=10).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[7] = prophet\n",
    "results['Prophet+ARIMA'].iloc[7] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchangeOT_10step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_10step_rate_mae.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
