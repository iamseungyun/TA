{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8851cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import pmdarima as pm\n",
    "import itertools\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, acf\n",
    "from statsmodels.graphics.tsaplots  import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.api import Holt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from arch import arch_model\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2e6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange = pd.read_csv('./data/exchange.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2cd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange['date'] = pd.to_datetime(exchange['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0951ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasting_horizon = int(exchange.shape[0]*0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e737f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['0', '1', '2', '3', '4', '5', '6', 'OT']\n",
    "columns = ['Actual', 'ARIMA', 'ARIMA 2',\\\n",
    "           'Simple Exponential Smoothing','Holt\\'s Exponential Smoothing', \\\n",
    "          'Prophet', 'Prophet+ARIMA']\n",
    "\n",
    "\n",
    "results = {}\n",
    "results = pd.DataFrame(results, index=index, columns =columns)\n",
    "results = results.drop(['Actual'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c0b6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Holt-Winter\\'s Exponential Smoothing', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97bd73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange0 = exchange[['date','0']]\n",
    "exchange1 = exchange[['date','1']]\n",
    "exchange2 = exchange[['date','2']]\n",
    "exchange3 = exchange[['date','3']]\n",
    "exchange4 = exchange[['date','4']]\n",
    "exchange5 = exchange[['date','5']]\n",
    "exchange6 = exchange[['date','6']]\n",
    "exchangeOT = exchange[['date','OT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "631ba512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_24540\\1671268679.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange0.rename(columns={'date':'ds', '0':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_24540\\1671268679.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange1.rename(columns={'date':'ds', '1':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_24540\\1671268679.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange2.rename(columns={'date':'ds', '2':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_24540\\1671268679.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange3.rename(columns={'date':'ds', '3':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_24540\\1671268679.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange4.rename(columns={'date':'ds', '4':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_24540\\1671268679.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange5.rename(columns={'date':'ds', '5':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_24540\\1671268679.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange6.rename(columns={'date':'ds', '6':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_24540\\1671268679.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchangeOT.rename(columns={'date':'ds', 'OT':'y'}, inplace =True)\n"
     ]
    }
   ],
   "source": [
    "exchange0.rename(columns={'date':'ds', '0':'y'}, inplace =True)\n",
    "exchange1.rename(columns={'date':'ds', '1':'y'}, inplace =True)\n",
    "exchange2.rename(columns={'date':'ds', '2':'y'}, inplace =True)\n",
    "exchange3.rename(columns={'date':'ds', '3':'y'}, inplace =True)\n",
    "exchange4.rename(columns={'date':'ds', '4':'y'}, inplace =True)\n",
    "exchange5.rename(columns={'date':'ds', '5':'y'}, inplace =True)\n",
    "exchange6.rename(columns={'date':'ds', '6':'y'}, inplace =True)\n",
    "exchangeOT.rename(columns={'date':'ds', 'OT':'y'}, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7f730",
   "metadata": {},
   "source": [
    "# 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea0b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6364482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[0] = arima\n",
    "#results['ARIMA-GARCH'].iloc[0] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcb03e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "#ARIMA(1,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[0] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[0] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae83c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(1).to_numpy().item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(1).to_numpy().item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(1).to_numpy().item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[0] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[0] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[0] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2864ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:31:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:32:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:32:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:32:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:32:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:32:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:32:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:32:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:33:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:33:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:33:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:33:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:33:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:33:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:34:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:34:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:34:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:34:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:34:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:34:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:35:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:35:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:35:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:35:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:35:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:35:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:36:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:36:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:36:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:36:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:36:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:36:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:37:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:37:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:37:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:37:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:37:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:37:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:38:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:38:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:38:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:38:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:38:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:38:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:39:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:39:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:39:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:39:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:39:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:39:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:40:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:40:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:40:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:40:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:40:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:40:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:41:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:41:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:41:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:41:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:41:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:41:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:42:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:42:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:42:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:42:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:42:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:42:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:43:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:43:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:43:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:43:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:43:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:43:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:44:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:44:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:44:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:44:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:44:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:44:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:45:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:45:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:45:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:45:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:45:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:45:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:46:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:46:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:46:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:46:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:46:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:46:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:47:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:47:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:47:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:47:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:47:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:47:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:48:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:48:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:48:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:48:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:49:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:49:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:49:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:50:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:50:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:50:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:50:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:50:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:50:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:50:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:51:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:51:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:51:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:52:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:52:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:52:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:53:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:53:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:53:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:53:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:53:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:53:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:54:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:54:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:54:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:54:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:54:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:54:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:55:08 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:55:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:55:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:55:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:55:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:55:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:56:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:56:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:56:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:56:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:56:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:57:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:57:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:57:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:57:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:57:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:58:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:58:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:58:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:58:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:58:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:58:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:58:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:59:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:59:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:59:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:59:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:59:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:59:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:59:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:00:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:00:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:00:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:00:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:00:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:00:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:01:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:01:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:01:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:02:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:02:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:02:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:02:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:02:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:02:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:03:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:03:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:03:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:04:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:04:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:04:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:04:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:04:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:04:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:06:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:06:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:06:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:06:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:06:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:06:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:06:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:07:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:07:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:07:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:07:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:07:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:07:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:08:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:08:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:08:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:08:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:08:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:08:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:08:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:09:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:09:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:09:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:09:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:09:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:09:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:10:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:10:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:10:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:10:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:10:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:10:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:11:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:11:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:11:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:11:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:11:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:11:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:12:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:12:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:12:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:12:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:12:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:12:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:13:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:13:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:13:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:13:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:13:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:13:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:14:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:14:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:14:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:14:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:14:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:14:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:16:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:16:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:16:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:16:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:16:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:16:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:17:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:17:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:17:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:17:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:17:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:17:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:18:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:18:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:18:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:18:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:18:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:18:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:19:11 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:19:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:19:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:19:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:19:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:19:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:20:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:20:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:20:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:20:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:20:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:20:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:21:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:21:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:21:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:21:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:21:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:21:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:22:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:22:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:22:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:22:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:22:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:22:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:23:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:23:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:23:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:23:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:23:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:23:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:24:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:24:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:24:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:24:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:24:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:24:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:25:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:25:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:25:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:25:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:25:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:25:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:25:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:26:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:26:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:26:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:26:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:26:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:26:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:27:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:27:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:27:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:27:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:27:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:27:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:28:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:28:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:28:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:28:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:28:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:28:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:28:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:29:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:29:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:30:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:30:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:30:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:31:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:31:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:31:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:35:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:35:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:35:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:37:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:37:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:37:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:37:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:37:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:37:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:14 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:42:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:53:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:53:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:53:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:57:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:57:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:57:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:59 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:04:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:04:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:04:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:08:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:08:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:08:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:08:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:08:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:08:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:09:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:09:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:09:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:09:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:09:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:09:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:10:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:10:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:10:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:11:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:11:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:11:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:11:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:11:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:11:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:12:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:12:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:12:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:12:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:12:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:12:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:13:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:13:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:13:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:13:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:13:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:13:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:14:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:14:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:14:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:14:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:15:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:15:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:15:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:15:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:15:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:16:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:16:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:16:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:16:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:16:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:17:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:17:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:17:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:17:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:17:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:17:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:18:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:18:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:18:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:18:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:18:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:18:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:19:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:19:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:19:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:19:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:19:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:19:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:20:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:20:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:20:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:20:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:20:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:20:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:21:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:21:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:21:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:21:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:21:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:21:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:22:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:22:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:22:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:22:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:22:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:22:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:23:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:23:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:23:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:23:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:23:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:23:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:24:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:24:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:24:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:24:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:24:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:24:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:25:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:25:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:25:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:25:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:25:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:25:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:26:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:26:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:26:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:26:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:26:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:27:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:27:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:27:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:27:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:27:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:27:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:28:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:28:31 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:28:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:28:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:29:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:29:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:29:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:29:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:29:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:29:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:30:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:30:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:30:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:30:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:30:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:30:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:31:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:31:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:31:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:31:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:31:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:31:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:32:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:32:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:32:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:32:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:32:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:32:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:32:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:32:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:33:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:33:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:33:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:33:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:33:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:33:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:34:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:34:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:34:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:34:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:34:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:34:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:37:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:37:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:37:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:37:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:37:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:37:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:38:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:38:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:38:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:38:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:38:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:38:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:39:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:39:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:39:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:39:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:39:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:39:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:40:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:40:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:40:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:40:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:40:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:40:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:44:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:44:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:44:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:45:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:45:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:45:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:45:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:45:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:45:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:46:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:46:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:46:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:47:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:47:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:47:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:47:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:47:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:47:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:48:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:48:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:48:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:49:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:49:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:49:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:49:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:49:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:49:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:49:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:50:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:50:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:50:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:50:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:50:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:50:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:35 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:51:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:52:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:52:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:52:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:52:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:52:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:52:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:54:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:54:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:54:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:55:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:55:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:55:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:55:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:55:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:55:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:56:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:56:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:56:20 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    }
   ],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-1]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=1).to_numpy().item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[0] = prophet\n",
    "results['Prophet+ARIMA'].iloc[0] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3acf9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange0_1step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1333121",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_1step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbdc271",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58019a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[1] = arima\n",
    "#results['ARIMA-GARCH'].iloc[1] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[1] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[1] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b41f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(1).to_numpy().item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(1).to_numpy().item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(1).to_numpy().item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[1] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[1] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[1] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-1]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=1).to_numpy().item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[1] = prophet\n",
    "results['Prophet+ARIMA'].iloc[1] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange1_1step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_1step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a090a",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[2] = arima\n",
    "#results['ARIMA-GARCH'].iloc[2] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c17f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(2,1,2)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(2,1,2)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (2,1,2)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[2] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[2] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(1).to_numpy().item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(1).to_numpy().item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(1).to_numpy().item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[2] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[2] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[2] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-1]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=1).to_numpy().item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[2] = prophet\n",
    "results['Prophet+ARIMA'].iloc[2] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange2_1step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_1step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa33e5",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28807527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(3,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(3,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (3,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[3] = arima\n",
    "#results['ARIMA-GARCH'].iloc[3] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ff695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,3)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,3)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,3)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[3] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[3] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(1).to_numpy().item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(1).to_numpy().item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(1).to_numpy().item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[3] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[3] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[3] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04512082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-1]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=1).to_numpy().item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[3] = prophet\n",
    "results['Prophet+ARIMA'].iloc[3] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange3_1step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e68fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_1step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e45b70",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf0239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[4] = arima\n",
    "#results['ARIMA-GARCH'].iloc[4] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47448064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,2,5)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,2,5)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,2,5)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[4] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[4] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84be666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(1).to_numpy().item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(1).to_numpy().item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(1).to_numpy().item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[4] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[4] = holt\n",
    "results['Holt-Winter\\'s Exponential Smoothing'].iloc[4] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfaa44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-1]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=1).to_numpy().item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[4] = prophet\n",
    "results['Prophet+ARIMA'].iloc[4] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b318fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange4_1step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_1step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dc1daf",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204020fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0085472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[5] = arima\n",
    "#results['ARIMA-GARCH'].iloc[5] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8296890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,3)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,3)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,3)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[5] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[5] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ab105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(1).to_numpy().item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(1).to_numpy().item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(1).to_numpy().item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[5] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[5] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[5] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee181b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-1]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=1).to_numpy().item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[5] = prophet\n",
    "results['Prophet+ARIMA'].iloc[5] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de20747",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange5_1step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_1step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1641a76",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb599bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "#ARIMA(0,1,1)-GARCH(1,1)\n",
    "arch_errors = []\n",
    "arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[6] = arima\n",
    "#results['ARIMA-GARCH'].iloc[6] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4097c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[6] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[6] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(1).to_numpy().item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(1).to_numpy().item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(1).to_numpy().item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[6] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[6] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[6] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-1]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=1).to_numpy().item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[6] = prophet\n",
    "results['Prophet+ARIMA'].iloc[6] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c69e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange6_1step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a51c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_1step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9a3bc",
   "metadata": {},
   "source": [
    "# OT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchangeOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8834ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[7] = arima\n",
    "#results['ARIMA-GARCH'].iloc[7] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=1).to_numpy().item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=1).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=1)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[7] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[7] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "#Holt-Winter\n",
    "hw_errors = []\n",
    "hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(1).to_numpy().item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(1).to_numpy().item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(1).to_numpy().item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[7] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[7] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[7] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-1]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=1).to_numpy().item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[7] = prophet\n",
    "results['Prophet+ARIMA'].iloc[7] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd02c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchangeOT_1step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_1step_rate_mae.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
