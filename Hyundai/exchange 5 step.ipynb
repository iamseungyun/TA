{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8851cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import pmdarima as pm\n",
    "import itertools\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, acf\n",
    "from statsmodels.graphics.tsaplots  import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.api import Holt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from arch import arch_model\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2e6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange = pd.read_csv('./data/exchange.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9dc2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange['date'] = pd.to_datetime(exchange['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0951ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasting_horizon = int(exchange.shape[0]*0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e737f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['0', '1', '2', '3', '4', '5', '6', 'OT']\n",
    "columns = ['Actual', 'ARIMA', 'ARIMA 2',\\\n",
    "           'Simple Exponential Smoothing','Holt\\'s Exponential Smoothing', \\\n",
    "          'Prophet', 'Prophet+ARIMA']\n",
    "\n",
    "\n",
    "results = {}\n",
    "results = pd.DataFrame(results, index=index, columns =columns)\n",
    "results = results.drop(['Actual'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97bd73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange0 = exchange[['date','0']]\n",
    "exchange1 = exchange[['date','1']]\n",
    "exchange2 = exchange[['date','2']]\n",
    "exchange3 = exchange[['date','3']]\n",
    "exchange4 = exchange[['date','4']]\n",
    "exchange5 = exchange[['date','5']]\n",
    "exchange6 = exchange[['date','6']]\n",
    "exchangeOT = exchange[['date','OT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "631ba512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_13420\\1671268679.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange0.rename(columns={'date':'ds', '0':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_13420\\1671268679.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange1.rename(columns={'date':'ds', '1':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_13420\\1671268679.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange2.rename(columns={'date':'ds', '2':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_13420\\1671268679.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange3.rename(columns={'date':'ds', '3':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_13420\\1671268679.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange4.rename(columns={'date':'ds', '4':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_13420\\1671268679.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange5.rename(columns={'date':'ds', '5':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_13420\\1671268679.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchange6.rename(columns={'date':'ds', '6':'y'}, inplace =True)\n",
      "C:\\Users\\Seungyun\\AppData\\Local\\Temp\\ipykernel_13420\\1671268679.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exchangeOT.rename(columns={'date':'ds', 'OT':'y'}, inplace =True)\n"
     ]
    }
   ],
   "source": [
    "exchange0.rename(columns={'date':'ds', '0':'y'}, inplace =True)\n",
    "exchange1.rename(columns={'date':'ds', '1':'y'}, inplace =True)\n",
    "exchange2.rename(columns={'date':'ds', '2':'y'}, inplace =True)\n",
    "exchange3.rename(columns={'date':'ds', '3':'y'}, inplace =True)\n",
    "exchange4.rename(columns={'date':'ds', '4':'y'}, inplace =True)\n",
    "exchange5.rename(columns={'date':'ds', '5':'y'}, inplace =True)\n",
    "exchange6.rename(columns={'date':'ds', '6':'y'}, inplace =True)\n",
    "exchangeOT.rename(columns={'date':'ds', 'OT':'y'}, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7f730",
   "metadata": {},
   "source": [
    "# 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f3f021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6364482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[0] = arima\n",
    "#results['ARIMA-GARCH'].iloc[0] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d5e9d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Seungyun\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "#ARIMA(1,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[0] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[0] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a59ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-4])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(5).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-4])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(5).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-4], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(5).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[0] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[0] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[0] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3eb870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:38:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:38:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:39:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:39:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:39:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:39:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:39:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:39:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:40:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:40:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:40:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:40:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:40:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:40:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:41:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:41:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:41:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:41:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:41:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:41:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:42:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:42:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:42:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:42:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:42:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:42:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:43:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:43:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:43:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:43:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:43:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:43:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:44:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:44:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:44:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:44:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:44:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:44:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:44:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:44:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:45:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:45:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:45:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:45:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:45:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:45:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:46:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:46:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:46:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:46:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:46:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:46:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:47:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:47:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:47:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:47:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:47:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:47:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:48:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:48:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:48:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:49:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:49:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:49:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:50:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:50:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:50:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:50:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:50:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:50:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:51:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:51:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:51:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:52:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:52:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:52:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:52:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:53:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:53:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:53:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:53:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:53:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:53:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:53:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:54:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:54:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:54:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:54:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:54:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:54:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:55:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:55:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:55:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:55:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:55:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:55:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:55:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:56:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:56:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:56:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:56:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:56:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:56:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:56:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:57:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:57:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:57:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:57:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:57:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:57:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:57:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:58:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:58:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:58:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:58:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:58:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:58:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:58:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:59:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:59:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:59:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:59:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:59:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:59:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:00:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:00:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:00:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:00:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:00:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:00:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:01:03 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:01:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:01:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:01:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:02:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:02:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:02:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:02:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:02:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:02:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:03:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:03:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:03:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:04:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:04:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:04:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:04:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:04:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:04:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:06:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:06:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:06:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:06:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:06:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:06:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:07:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:07:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:07:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:07:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:07:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:07:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:08:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:08:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:08:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:08:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:08:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:08:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:09:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:09:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:09:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:09:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:09:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:09:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:10:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:10:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:10:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:10:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:10:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:10:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:11:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:11:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:11:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:11:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:11:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:11:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:11:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:12:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:12:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:12:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:12:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:12:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:12:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:12:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:13:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:13:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:13:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:13:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:13:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:13:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:14:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:14:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:14:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:14:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:14:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:14:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:16:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:16:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:16:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:16:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:16:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:16:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:17:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:17:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:17:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:17:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:17:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:17:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:18:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:18:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:18:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:18:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:18:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:18:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:19:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:19:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:19:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:19:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:19:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:19:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:20:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:20:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:20:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:20:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:20:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:20:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:20:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:21:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:21:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:21:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:21:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:21:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:21:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:21:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:22:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:22:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:22:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:22:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:22:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:22:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:23:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:23:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:23:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:23:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:23:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:23:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:24:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:24:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:24:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:24:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:24:56 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:24:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:25:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:25:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:25:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:25:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:25:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:25:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:26:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:26:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:26:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:26:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:26:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:26:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:27:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:27:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:27:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:27:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:27:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:27:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:28:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:28:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:28:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:28:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:28:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:28:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:29:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:29:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:29:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:30:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:30:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:30:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:31:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:31:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:31:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:35:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:35:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:35:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:37:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:37:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:37:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:37:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:37:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:37:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:57 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:48:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:53:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:53:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:53:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:53:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:57:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:57:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:57:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:04:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:04:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:04:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:08:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:08:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:08:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:08:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:08:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:08:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:08:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:09:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:09:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:09:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:09:34 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:09:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:09:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:10:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:10:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:10:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:11:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:11:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:11:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:11:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:11:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:11:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:12:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:12:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:12:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:12:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:12:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:12:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:13:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:13:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:13:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:13:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:13:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:13:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:14:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:14:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:14:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:15:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:15:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:15:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:15:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:15:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:16:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:16:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:16:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:16:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:16:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:16:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:16:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:17:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:17:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:17:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:17:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:17:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:17:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:18:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:18:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:18:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:18:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:18:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:18:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:19:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:19:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:19:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:19:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:19:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:19:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:20:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:20:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:20:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:20:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:20:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:20:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:21:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:21:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:21:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:21:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:21:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:21:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:21:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:22:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:22:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:22:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:22:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:22:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:22:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:22:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:23:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:23:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:23:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:23:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:23:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:24:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:24:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:24:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:24:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:24:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:24:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:24:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:25:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:25:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:25:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:25:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:25:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:26:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:26:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:26:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:26:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:26:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:27:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:27:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:27:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:27:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:27:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:27:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:28:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:28:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:28:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:29:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:29:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:29:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:29:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:29:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:29:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:30:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:30:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:30:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:30:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:30:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:30:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:31:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:31:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:31:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:31:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:31:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:31:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:32:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:32:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:32:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:32:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:32:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:32:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:33:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:33:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:33:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:33:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:33:54 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:33:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:34:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:34:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:34:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:34:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:34:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:34:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:37:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:37:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:37:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:37:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:37:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:37:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:38:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:38:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:38:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:38:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:38:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:38:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:38:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:38:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:39:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:39:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:39:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:39:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:39:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:39:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:40:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:40:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:40:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:40:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:40:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:40:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:44:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:44:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:44:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:45:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:45:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:45:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:45:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:45:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:45:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:46:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:46:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:46:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:47:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:47:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:47:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:47:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:47:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:47:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:47:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:48:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:48:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:48:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:49:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:49:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:49:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:49:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:49:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:49:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:50:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:50:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:50:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:50:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:50:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:50:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:52:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:52:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:52:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:52:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:52:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:52:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:54:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:54:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:54:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:55:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:55:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:55:28 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    }
   ],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-5]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-4])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-4] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=5).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[0] = prophet\n",
    "results['Prophet+ARIMA'].iloc[0] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac004cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange0_5step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_5step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65620694",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b3ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[1] = arima\n",
    "#results['ARIMA-GARCH'].iloc[1] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f57d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[1] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[1] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4777950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-4])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(5).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-4])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(5).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-4], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(5).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[1] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[1] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[1] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13670b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-5]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-4])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-4] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=5).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[1] = prophet\n",
    "results['Prophet+ARIMA'].iloc[1] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f211a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange1_5step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb162da",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_5step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3b074",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[2] = arima\n",
    "#results['ARIMA-GARCH'].iloc[2] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(2,1,2)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(2,1,2)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (2,1,2)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[2] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[2] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e673ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-4])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(5).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-4])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(5).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-4], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(5).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[2] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[2] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[2] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-5]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-4])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-4] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=5).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[2] = prophet\n",
    "results['Prophet+ARIMA'].iloc[2] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange2_5step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_5step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e8fa6",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4475c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(3,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(3,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (3,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[3] = arima\n",
    "#results['ARIMA-GARCH'].iloc[3] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,3)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,3)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,3)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[3] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[3] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653515b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-4])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(5).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-4])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(5).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-4], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(5).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[3] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[3] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[3] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-5]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-4])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-4] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=5).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[3] = prophet\n",
    "results['Prophet+ARIMA'].iloc[3] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfad429",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange3_5step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ac366",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_5step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce946a",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6545129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[4] = arima\n",
    "#results['ARIMA-GARCH'].iloc[4] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86136ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,2,5)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,2,5)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,2,5)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[4] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[4] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-4])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(5).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-4])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(5).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-4], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(5).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[4] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[4] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[4] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-5]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-4])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-4] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=5).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[4] = prophet\n",
    "results['Prophet+ARIMA'].iloc[4] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange4_5step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_5step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45164944",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "arch_abs_errors = np.array(arch_errors)\n",
    "arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[5] = arima\n",
    "#results['ARIMA-GARCH'].iloc[5] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,3)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,3)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,3)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[5] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[5] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3cadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-4])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(5).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-4])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(5).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-4], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(5).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[5] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[5] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[5] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-5]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-4])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-4] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=5).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[5] = prophet\n",
    "results['Prophet+ARIMA'].iloc[5] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange5_5step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb957a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_5step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eeb0fa",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e6aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchange6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[6] = arima\n",
    "#results['ARIMA-GARCH'].iloc[6] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "# arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[6] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[6] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20174eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "#Holt-Winter\n",
    "hw_errors = []\n",
    "hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-4])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(5).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-4])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(5).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-4], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(5).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[6] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[6] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[6] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aae3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-5]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-4])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-4] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=5).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[6] = prophet\n",
    "results['Prophet+ARIMA'].iloc[6] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5590563",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchange6_5step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_5step_rate_mae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f1426",
   "metadata": {},
   "source": [
    "# OT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exchangeOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(0,1,1)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(0,1,1)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (0,1,1)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA'].iloc[7] = arima\n",
    "#results['ARIMA-GARCH'].iloc[7] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA(1,1,0)\n",
    "arima_errors = []\n",
    "arima_preds1 = []\n",
    "# #ARIMA(1,1,0)-GARCH(1,1)\n",
    "# arch_errors = []\n",
    "# arch_preds1 = []\n",
    "order = (1,1,0)\n",
    "\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    arima_model = ARIMA(data['y'][:target-4], order = order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_pred = arima_model_fit.forecast(steps=5).iloc[-1].item()\n",
    "    arima_error = arima_pred - data['y'][target]\n",
    "    arima_abs_error = abs(arima_error)\n",
    "    arima_preds1.append(arima_pred)\n",
    "    arima_errors.append(arima_abs_error)\n",
    "    \n",
    "#     arima_residuals = arima_model_fit.resid\n",
    "#     garch = arch_model(arima_residuals, p=1,q=1)\n",
    "#     garch_fitted = garch.fit()\n",
    "#     predicted_mu = arima_model_fit.predict(n_periods=5).iloc[-1]\n",
    "#     garch_forecast = garch_fitted.forecast(horizon=5)\n",
    "#     predicted_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "#     arch_pred = predicted_mu + predicted_et\n",
    "#     arch_error = arch_pred - data['y'][target]\n",
    "#     arch_abs_error = abs(arch_error)\n",
    "#     arch_preds1.append(arch_pred)\n",
    "#     arch_errors.append(arch_abs_error)\n",
    "\n",
    "    \n",
    "arima_abs_errors = np.array(arima_errors)\n",
    "arima = arima_abs_errors.mean()\n",
    "# arch_abs_errors = np.array(arch_errors)\n",
    "# arch = arch_abs_errors.mean()\n",
    "\n",
    "results['ARIMA 2'].iloc[7] = arima\n",
    "#results['ARIMA-GARCH 2'].iloc[7] = arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SES\n",
    "ses_errors = []\n",
    "ses_preds1 = []\n",
    "#Holt\n",
    "holt_errors = []\n",
    "holt_preds1 = []\n",
    "# #Holt-Winter\n",
    "# hw_errors = []\n",
    "# hw_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    ses_model = SimpleExpSmoothing(data['y'][:target-4])\n",
    "    ses_model_fit = ses_model.fit()\n",
    "    ses_pred = ses_model_fit.forecast(5).iloc[-1].item()\n",
    "    ses_error = ses_pred - data['y'][target]\n",
    "    ses_abs_error = abs(ses_error)\n",
    "    ses_preds1.append(ses_pred)\n",
    "    ses_errors.append(ses_abs_error)\n",
    "    \n",
    "    holt_model = Holt(data['y'][:target-4])\n",
    "    holt_model_fit = holt_model.fit()\n",
    "    holt_pred = holt_model_fit.forecast(5).iloc[-1].item()\n",
    "    holt_error = holt_pred - data['y'][target]\n",
    "    holt_abs_error = abs(holt_error)\n",
    "    holt_preds1.append(holt_pred)\n",
    "    holt_errors.append(holt_abs_error)\n",
    "    \n",
    "#     hw_model = ExponentialSmoothing(data['y'][:-target-4], seasonal_periods=365, trend='add', seasonal='add')\n",
    "#     hw_model_fit = hw_model.fit()\n",
    "#     hw_pred = hw_model_fit.forecast(5).iloc[-1].item()\n",
    "#     hw_error = hw_pred - data['y'][target]\n",
    "#     hw_abs_error = abs(hw_error)\n",
    "#     hw_preds1.append(hw_pred)\n",
    "#     hw_errors.append(hw_abs_error)\n",
    "    \n",
    "ses_abs_errors = np.array(ses_errors)\n",
    "ses = ses_abs_errors.mean()\n",
    "holt_abs_errors = np.array(holt_errors)\n",
    "holt = holt_abs_errors.mean()\n",
    "# hw_abs_errors = np.array(hw_errors)\n",
    "# hw = hw_abs_errors.mean()\n",
    "\n",
    "results['Simple Exponential Smoothing'].iloc[7] = ses\n",
    "results['Holt\\'s Exponential Smoothing'].iloc[7] = holt\n",
    "#results['Holt-Winter\\'s Exponential Smoothing'].iloc[7] = hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prophet\n",
    "prophet_errors = []\n",
    "prophet_preds1 = []\n",
    "#prophet-ARIMA\n",
    "prophet_arima_errors = []\n",
    "prophet_arima_preds1 = []\n",
    "\n",
    "for i in range(forecasting_horizon) :\n",
    "    target = data.shape[0]-forecasting_horizon+i\n",
    "    date = data['ds'].iloc[target] - data['ds'].iloc[target-5]\n",
    "    m = Prophet()\n",
    "    m.fit(data.iloc[:target-4])\n",
    "    future = m.make_future_dataframe(periods=date.days)\n",
    "    forecast = m.predict(future)\n",
    "    prophet_pred = forecast['yhat'].iloc[-1]\n",
    "    prophet_error = prophet_pred - data['y'][target]\n",
    "    prophet_abs_error = abs(prophet_error)\n",
    "    prophet_preds1.append(prophet_pred)\n",
    "    prophet_errors.append(prophet_abs_error)\n",
    "    \n",
    "    \n",
    "    error_series = data['y'][:target-4] - forecast['yhat'].iloc[:-date.days]\n",
    "    prophet_arima_model = pm.auto_arima(error_series, start_p = 0, start_q = 0, max_p = 5, max_q = 5, \\\n",
    "                                        seasonal = False, information_criteria = 'bic')\n",
    "    prophet_arima_pred = prophet_arima_model.predict(n_periods=5).iloc[-1].item() + prophet_pred\n",
    "    prophet_arima_error = prophet_arima_pred - data['y'][target]\n",
    "    prophet_arima_abs_error = abs(prophet_arima_error)\n",
    "    prophet_arima_preds1.append(prophet_arima_pred)\n",
    "    prophet_arima_errors.append(prophet_arima_abs_error)    \n",
    "\n",
    "    \n",
    "prophet_abs_errors = np.array(prophet_errors)\n",
    "prophet = prophet_abs_errors.mean()\n",
    "prophet_arima_abs_errors = np.array(prophet_arima_errors)\n",
    "prophet_arima = prophet_arima_abs_errors.mean()\n",
    "\n",
    "results['Prophet'].iloc[7] = prophet\n",
    "results['Prophet+ARIMA'].iloc[7] = prophet_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred =pd.DataFrame(index= data['ds'][-forecasting_horizon:], columns=columns)\n",
    "data_pred['Actual'] = list(data['y'][data.shape[0]-forecasting_horizon:])\n",
    "data_pred['ARIMA'] = arima_preds1\n",
    "#data_pred['ARIMA-GARCH'] = arch_preds1\n",
    "data_pred['ARIMA 2'] = arima_preds2_1\n",
    "#data_pred['ARIMA-GARCH 2'] = arch_preds2_1\n",
    "data_pred['Simple Exponential Smoothing'] = ses_preds1\n",
    "data_pred['Holt\\'s Exponential Smoothing'] = holt_preds1\n",
    "#data_pred['Holt-Winter\\'s Exponential Smoothing'] = hw_preds1\n",
    "data_pred['Prophet'] = prophet_preds1\n",
    "data_pred['Prophet+ARIMA'] = prophet_arima_preds1\n",
    "data_pred.to_csv('./results/exchangeOT_5step_forecasting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/exchange_5step_rate_mae.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
